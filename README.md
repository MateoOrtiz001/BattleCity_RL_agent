# BattleCity RL Agent
<div align="center">
<img src="assets/banner.gif" alt="Banner">
</div>
Agente de Aprendizaje por Refuerzo (Q-Learning) para el videojuego cl√°sico BattleCity. El proyecto implementa tanto Q-Learning tabular como aproximado con caracter√≠sticas lineales.


[![V√≠deo de Muestra](https://img.youtube.com/vi/nLUwcIf3_e8/sddefault.jpg)](https://youtube.com/shorts/nLUwcIf3_e8)
## Descripci√≥n

Este proyecto entrena un agente inteligente para jugar una versi√≥n reducida de BattleCity, donde el objetivo es destruir todos los tanques enemigos mientras se protege la base aliada. El agente aprende a trav√©s de la interacci√≥n con el entorno, optimizando sus decisiones para maximizar la recompensa acumulada.

### Caracter√≠sticas principales

- **Q-Learning Tabular**: Aprende valores Q exactos para cada par estado-acci√≥n
- **Q-Learning Aproximado**: Usa caracter√≠sticas lineales para generalizar a estados no vistos
- **Visualizaci√≥n con Pygame**: Observa el comportamiento del agente en tiempo real
- **An√°lisis de Cadenas de Markov**: Extrae y analiza la pol√≠tica aprendida

## Requisitos

```bash
pip install pygame numpy
```

## Estructura del Proyecto

```
BattleCity_RL_agent/
‚îú‚îÄ‚îÄ train_agent.py          # Script de entrenamiento
‚îú‚îÄ‚îÄ play_game.py            # Visualizaci√≥n de partidas
‚îú‚îÄ‚îÄ extract_markov_chain.py # An√°lisis de la pol√≠tica
‚îú‚îÄ‚îÄ models/                 # Agentes entrenados (.pkl)
‚îî‚îÄ‚îÄ src/
    ‚îú‚îÄ‚îÄ agents/             # Implementaciones de agentes RL
    ‚îú‚îÄ‚îÄ gameClass/          # L√≥gica del juego y estados
    ‚îú‚îÄ‚îÄ training/           # Entorno y trainer
    ‚îî‚îÄ‚îÄ utils/              # Utilidades
```

## Entrenamiento

### Modos predefinidos

```bash
# Demo r√°pido (100 episodios)
python train_agent.py --mode demo

# Entrenamiento b√°sico (1000 episodios)
python train_agent.py --mode basic

# Entrenamiento con aproximaci√≥n de funciones (2000 episodios)
python train_agent.py --mode approximate
```

### Entrenamiento personalizado

```bash
python train_agent.py --episodes 5000 --epsilon 0.3 --alpha 0.2 --gamma 0.9 --level 1
```

### Par√°metros disponibles

| Par√°metro | Descripci√≥n | Default |
|-----------|-------------|---------|
| `--episodes` | N√∫mero de episodios de entrenamiento | 1000 |
| `--epsilon` | Tasa de exploraci√≥n (Œµ-greedy) | 0.3 |
| `--alpha` | Tasa de aprendizaje | 0.2 |
| `--gamma` | Factor de descuento | 0.9 |
| `--level` | Nivel del juego (1-4) | 1 |
| `--approximate` | Usar ApproximateQAgent | False |
| `--save` | Ruta para guardar el modelo | `models/agent` |

### Resultados esperados

Con ~1000 episodios:
- Tasa de victorias: 65-75%
- Tiempo de entrenamiento: ~2 minutos

## Visualizaci√≥n de Partidas

### Uso b√°sico

```bash
# Ejecutar con configuraci√≥n por defecto
python play_game.py

# Especificar un agente entrenado
python play_game.py --agent models/mi_agente.pkl

# Modo texto (sin ventana gr√°fica)
python play_game.py --text
```

### Opciones de visualizaci√≥n

| Opci√≥n | Descripci√≥n | Default |
|--------|-------------|---------|
| `--agent` | Ruta al agente entrenado | `models/qlearning_basic_final.pkl` |
| `--level` | Nivel del juego (1-4) | 1 |
| `--games` | N√∫mero de partidas | 1 |
| `--delay` | Delay entre frames (ms) | 150 |
| `--approximate` | Usar agente aproximado | False |
| `--text` | Modo texto | False |

### Ejemplos

```bash
# Partida lenta para analizar
python play_game.py --delay 300

# 10 partidas consecutivas
python play_game.py --games 10

# Usar agente aproximado
python play_game.py --agent models/approximate_final.pkl --approximate
```

### Controles (Pygame)

| Tecla | Acci√≥n |
|-------|--------|
| `ESC` | Salir |
| `SPACE` | Pausar/Reanudar |
| `+` / `-` | Ajustar velocidad |
| `ENTER` | Siguiente partida |

### Elementos visuales

- üü© **Verde**: Tu tanque (con barra de vida)
- üü• **Rojo**: Tanques enemigos
- üü® **Dorado**: Base a proteger
- üü´ **Marr√≥n**: Muros destructibles
- ‚¨ú **Gris**: Muros indestructibles

## Sistema de Recompensas

| Evento | Recompensa |
|--------|------------|
| Victoria | +1000 |
| Derrota | -500 |
| Eliminar enemigo | +100 |
| Perder vida | -20 |
| Disparar | +5 |
| Acercarse al enemigo | +2 |
| Cada paso (tiempo) | -1 |

## An√°lisis de la Pol√≠tica

Extrae la cadena de Markov de la pol√≠tica aprendida:

```bash
python extract_markov_chain.py --agent models/mi_agente.pkl --episodes 500
```

Esto permite analizar:
- Matriz de transici√≥n de estados
- Distribuci√≥n estacionaria
- Probabilidades de victoria/derrota
- Tiempos esperados de absorci√≥n

## Licencia

Ver archivo [LICENSE](LICENSE) para m√°s detalles.
